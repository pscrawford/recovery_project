{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options for encoding ordinal response\n",
    "\n",
    "- Naive categorical encoding (ignore order)\n",
    "- Integer encoding (really regression not classification)\n",
    "     - For numpy array, use sklearn's LabelEncoder (string -> integer)\n",
    "     - For pandas DataFrame, can try OrdinalEncoder in the [Category Encoders package](http://contrib.scikit-learn.org/categorical-encoding/index.html)\n",
    "- [Ordinal crossentropy loss for Keras](https://github.com/JHart96/keras_ordinal_categorical_crossentropy)\n",
    "- [Ordinal regression for TF](https://github.com/gspell/TF-OrdinalRegression)\n",
    "- \"Cumulative\" encoding [Cheng et al.]\n",
    "     - [Creating custom encoders](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)\n",
    "- Split into K-1 binary classification problems [Frank and Hall] \n",
    "     - not sure if it's efficient with neural nets\n",
    "     - Cheng et al.'s encoding does this in some sense, within a single network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "# if installed, keras uses tf as backend\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.optimizers import Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "from sklearn import model_selection\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Import ordinal crossentropy loss function\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from keras_ordinal_categorical_crossentropy import ordinal_categorical_crossentropy as OCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated ordinal response\n",
    "\n",
    "Let's generate toy data for which we actually know the \n",
    "data generating process (DGP). This will provide a better\n",
    "benchmark for the different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0. set rng seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# 1. set parameters\n",
    "\n",
    "K = 3 # response categories\n",
    "N = 10000 # number of examples\n",
    "P = 3 # number of features\n",
    "\n",
    "# thresholds:\n",
    "mu0 = 0\n",
    "mu1 = 3.14\n",
    "\n",
    "# set DGP parameters\n",
    "b0 = 1\n",
    "B = np.random.randint(low=-3, high=3, size=(P,1)) # dim: Px1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. generate features\n",
    "x_mean = np.random.normal(size=P) # dim: Px1\n",
    "\n",
    "x_cov = np.identity(P)\n",
    "# TODO: automate correlation\n",
    "x_cov[1,0], x_cov[0,1] = 0.2, 0.2\n",
    "x_cov[2,0], x_cov[0,2] = -0.1, -0.1\n",
    "x_cov[1,2], x_cov[2,1] = 0.5, 0.5\n",
    "\n",
    "X = np.random.multivariate_normal(x_mean, x_cov, N) # dim: NxP\n",
    "# X.shape\n",
    "\n",
    "# TODO: generate categorical features\n",
    "# - X[:, 0] = np.digitize(X[:, 0], [-0.5, 0.0, 0.5], right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. generate normal error\n",
    "u = np.random.normal(size=(N,1)) # dim: Nx1\n",
    "# u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. generate latent response\n",
    "y_latent = b0 + X.dot(B) + u # dim: Nx1\n",
    "# y_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.104424045989525, 3.783827357238667, 3.7843600827217534, 12.182256525483465]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.min(y_latent), np.mean(y_latent), np.median(y_latent), np.max(y_latent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. generate observed ordinal response\n",
    "y = np.digitize(y_latent, [mu0, mu1], right=1)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    2],\n",
       "       [ 425, 3433, 6142]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. one-hot encoding\n",
    "Y = keras.utils.to_categorical(y, num_classes=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for k-fold CV\n",
    "n_folds = 10 # default is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Categorical encoding\n",
    "\n",
    "Comparison of categorical cross-entropy vs ordinal cross-entropy suggests\n",
    "ordinal cross-entropy is no better than categorical.\n",
    "\n",
    "### Hyperparameters to test\n",
    "\n",
    "- number of layers\n",
    "  - deeper is better than shallow\n",
    "- number of nodes: problem specific? \n",
    "  - wider input layer is better than narrow\n",
    "  - have not tested hidden layer width\n",
    "- activations\n",
    "  - input: relu\n",
    "  - hidden: tanh\n",
    "  - output: depends on encoding/loss (eg, cross-entropy -> softmax; mse -> relu)\n",
    "- dropout\n",
    "  - dropout on input **and** hidden layers is better than no dropout\n",
    "  - dropout rate 0.5 better than smaller rate\n",
    "- initializer\n",
    "  - kernel: glorot_uniform better than glorot_normal\n",
    "  - bias:**TODO**\n",
    "- optimizer\n",
    "  - sgd is best so far\n",
    "- epochs\n",
    "  - 200\n",
    "- batch\n",
    "  - 128\n",
    "- **TODO**: other types of layers (LeakyReLu, PReLu, ELU, ThresholdedReLU)\n",
    "- **TODO**: other regularization: activation reg not good\n",
    "\n",
    "\n",
    "Another major **TODO**: model performance with categorical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_base(optimizer='sgd', init='glorot_uniform',\n",
    "                 n_input=64, n_hidden=64, dropout=0.5, p=1, k=1,\n",
    "                 input_act='relu', hidden_act='relu', output_act='softmax',\n",
    "                 loss='categorical_crossentropy'):\n",
    "    # create model w/ two hidden layers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_input, activation=input_act, input_dim=p, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_hidden, activation=hidden_act, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_hidden, activation=hidden_act, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))    \n",
    "    model.add(Dense(k, activation=output_act, kernel_initializer=init))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_base, p=P, k=K, n_input=10*P, \n",
    "                        batch_size=16, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search epochs, batch size, initializer, and optimizer\n",
    "input_acts = ['relu', 'tanh', 'sigmoid']\n",
    "hidden_acts = ['relu', 'tanh', 'sigmoid']\n",
    "param_grid = dict(input_act=input_acts, hidden_act=hidden_acts)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=n_folds)\n",
    "\n",
    "# one-hot encoding\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.829700 using {'hidden_act': 'tanh', 'input_act': 'relu'}\n",
      "0.810100 (0.015398) with: {'hidden_act': 'relu', 'input_act': 'relu'}\n",
      "0.821300 (0.017527) with: {'hidden_act': 'relu', 'input_act': 'tanh'}\n",
      "0.771800 (0.031505) with: {'hidden_act': 'relu', 'input_act': 'sigmoid'}\n",
      "0.829700 (0.013062) with: {'hidden_act': 'tanh', 'input_act': 'relu'}\n",
      "0.826400 (0.016169) with: {'hidden_act': 'tanh', 'input_act': 'tanh'}\n",
      "0.828800 (0.015025) with: {'hidden_act': 'tanh', 'input_act': 'sigmoid'}\n",
      "0.817900 (0.015839) with: {'hidden_act': 'sigmoid', 'input_act': 'relu'}\n",
      "0.819300 (0.014792) with: {'hidden_act': 'sigmoid', 'input_act': 'tanh'}\n",
      "0.710900 (0.069358) with: {'hidden_act': 'sigmoid', 'input_act': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now estimate best model using ordinal cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=create_base, p=P, k=K, n_input=10*P, input_act='tanh',\n",
    "                            loss=OCC.loss, batch_size=16, epochs=100, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.34% (1.39%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid parameter search (NB: this was run before the preceding code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model(optimizer='sgd', init='glorot_uniform',,\n",
    "                 n_input=64, n_hidden=64, dropout=0.5,\n",
    "                 p=1, k=1, input_act='relu', output_act='softmax',\n",
    "                 loss='categorical_crossentropy'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_input, activation=input_act, input_dim=p, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_hidden, activation=input_act, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(k, activation=output_act, kernel_initializer=init))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, p=P, k=K, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid search epochs, batch size, initializer, and optimizer\n",
    "## loss = ['categorical_crossentropy', 'ordinal_crossentropy']\n",
    "## optimizers = ['sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam']\n",
    "## inits = ['glorot_uniform', 'glorot_normal', 'uniform', 'normal']\n",
    "optimizers = ['sgd', 'rmsprop']\n",
    "inits = ['glorot_uniform', 'glorot_normal']\n",
    "epochs = [20, 200]\n",
    "batches = [16, 128]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=n_folds)\n",
    "\n",
    "# one-hot encoding\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.849800 using {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.849500 (0.008631) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.846100 (0.009118) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.848100 (0.009447) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.849400 (0.008952) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.843600 (0.010627) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.847100 (0.011905) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.846800 (0.007019) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.845200 (0.010980) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.841100 (0.009521) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.846400 (0.009297) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.842500 (0.006663) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.849000 (0.008373) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.849800 (0.007646) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.844300 (0.007769) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.848800 (0.008280) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.846400 (0.007946) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: how to pass callable OCC.loss to CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Integer encoding\n",
    "\n",
    "Compare cross entropy vs mse; output dim = 1 and output activation is relu (or sigmoid)\n",
    "\n",
    "**TODO**: create pipeline that first compiles and tests model with categorical encoding, then compiles and tests model with integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tf/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: You are passing a target array of shape (9000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n",
      "```\n",
      "from keras.utils import to_categorical\n",
      "y_binary = to_categorical(y_int)\n",
      "```\n",
      "\n",
      "Alternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "losses = ['categorical_crossentropy', 'mse']\n",
    "param_grid = dict(loss=losses)\n",
    "\n",
    "# classifier or regressor?\n",
    "model_int = KerasClassifier(build_fn=create_base, p=P, k=1, n_input=10*P,\n",
    "                            input_act='tanh', output_act='relu', \n",
    "                            batch_size=16, epochs=100, verbose=0)\n",
    "\n",
    "grid_int = GridSearchCV(estimator=model_int, param_grid=param_grid, cv=n_folds)\n",
    "\n",
    "# integer encoding\n",
    "grid_int_result = grid_int.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.830400 using {'hidden_act': 'tanh', 'input_act': 'relu'}\n",
      "nan (nan) with: {'loss': 'categorical_crossentropy'}\n",
      "0.830400 (0.015396) with: {'loss': 'mse'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_int_result.best_score_, grid_result.best_params_))\n",
    "means = grid_int_result.cv_results_['mean_test_score']\n",
    "stds = grid_int_result.cv_results_['std_test_score']\n",
    "params = grid_int_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sparse integer encoding\n",
    "\n",
    "Dimension of output layer is K, activation is relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ordinal crossentropy loss\n",
    "model.compile(loss=OCC.loss,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y_cat, epochs=20, batch_size=128)\n",
    "score = model.evaluate(test_x, test_y_cat, batch_size=128)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Sparse integer encoding is actually the worst!\n",
    "- Accuracy about the same for cross_entropy and ordinal_cross_entropy loss functions + categorical encoding\n",
    "\n",
    "This is still using the default network settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TODO: spatial correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
