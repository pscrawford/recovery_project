{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options for encoding ordinal response\n",
    "\n",
    "- Naive categorical encoding (ignore order)\n",
    "- Integer encoding (really regression not classification)\n",
    "     - For numpy array, use sklearn's LabelEncoder (string -> integer)\n",
    "     - For pandas DataFrame, can try OrdinalEncoder in the [Category Encoders package](http://contrib.scikit-learn.org/categorical-encoding/index.html)\n",
    "- [Ordinal crossentropy loss for Keras](https://github.com/JHart96/keras_ordinal_categorical_crossentropy)\n",
    "- [Ordinal regression for TF](https://github.com/gspell/TF-OrdinalRegression)\n",
    "- \"Cumulative\" encoding [Cheng et al.]\n",
    "     - [Creating custom encoders](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)\n",
    "- Split into K-1 binary classification problems [Frank and Hall] \n",
    "     - not sure if it's efficient with neural nets\n",
    "     - Cheng et al.'s encoding does this in some sense, within a single network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "# if installed, keras uses tf as backend\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.optimizers import Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "from sklearn import model_selection\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Import ordinal crossentropy loss function\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from keras_ordinal_categorical_crossentropy import ordinal_categorical_crossentropy as OCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated ordinal response\n",
    "\n",
    "Let's generate toy data for which we actually know the \n",
    "data generating process (DGP). This will provide a better\n",
    "benchmark for the different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0. set rng seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# 1. set parameters\n",
    "\n",
    "K = 3 # response categories\n",
    "N = 10000 # number of examples\n",
    "P = 3 # number of features\n",
    "\n",
    "# thresholds:\n",
    "mu0 = 0\n",
    "mu1 = 3.14\n",
    "\n",
    "# set DGP parameters\n",
    "b0 = 1\n",
    "b1 = 2\n",
    "b2 = -2\n",
    "b3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. generate features\n",
    "# TODO: auto-generate x_mean and x_cov\n",
    "# TODO: generate categorical features\n",
    "x_mean = (1, 2, 0.5) \n",
    "x_cov = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "X = np.random.multivariate_normal(x_mean, x_cov, N) # dim: NxK\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. generate normal error\n",
    "u = np.random.normal(size=(N,1)) # dim: Nx1\n",
    "# u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. generate latent response\n",
    "B = np.array([[b1], [b2], [b3]]) # dim: Kx1\n",
    "y_latent = b0 + X.dot(B) + u # dim: Nx1\n",
    "# y_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-13.413067400889629,\n",
       " -0.4856418920579942,\n",
       " -0.4903702239051473,\n",
       " 11.038336261288451]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.min(y_latent), np.mean(y_latent), np.median(y_latent), np.max(y_latent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. generate ordinal response\n",
    "y = np.digitize(y_latent, [mu0, mu1], right=1)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    2],\n",
       "       [5622, 3149, 1229]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. one-hot encoding\n",
    "Y = keras.utils.to_categorical(y, num_classes=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train / test split\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X,y,test_size = 0.1, random_state = 0)\n",
    "\n",
    "# encode response as categorical\n",
    "train_y_cat = keras.utils.to_categorical(train_y, num_classes=K)\n",
    "test_y_cat = keras.utils.to_categorical(test_y, num_classes=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for k-fold CV\n",
    "n_folds = 10 # default is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters to test\n",
    "\n",
    "- number of layers: more layers not better?\n",
    "- other types of layers (LeakyReLu, PReLu, ELU, ThresholdedReLU):\n",
    "- number of nodes: problem specific? more nodes not better...\n",
    "- activations: tanh, sigmoid slightly better than relu. others?\n",
    "- dropout: smaller dropout slightly better?\n",
    "- other regularization: activation reg not good; \n",
    "- initializer (kernel, bias)\n",
    "- optimizer\n",
    "- epochs\n",
    "- batch\n",
    "\n",
    "Let's begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Categorical encoding\n",
    "\n",
    "Compare categorical cross-entropy vs ordinal cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model(optimizer='sgd', init='glorot_uniform',,\n",
    "                 n_input=64, n_hidden=64, dropout=0.5,\n",
    "                 p=1, k=1, input_act='relu', output_act='softmax',\n",
    "                 loss='categorical_crossentropy'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_input, activation=input_act, input_dim=p, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_hidden, activation=input_act, kernel_initializer=init))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(k, activation=output_act, kernel_initializer=init))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, p=P, k=K, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search epochs, batch size, initializer, and optimizer\n",
    "## loss = ['categorical_crossentropy', 'ordinal_crossentropy']\n",
    "## optimizers = ['sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam']\n",
    "## inits = ['glorot_uniform', 'glorot_normal', 'uniform', 'normal']\n",
    "optimizers = ['sgd', 'rmsprop']\n",
    "inits = ['glorot_uniform', 'glorot_normal']\n",
    "epochs = [20, 200]\n",
    "batches = [16, 128]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=n_folds)\n",
    "\n",
    "# one-hot encoding\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.849800 using {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.849500 (0.008631) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.846100 (0.009118) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.848100 (0.009447) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.849400 (0.008952) with: {'batch_size': 16, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.843600 (0.010627) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.847100 (0.011905) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.846800 (0.007019) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.845200 (0.010980) with: {'batch_size': 16, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.841100 (0.009521) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.846400 (0.009297) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.842500 (0.006663) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.849000 (0.008373) with: {'batch_size': 128, 'epochs': 20, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n",
      "0.849800 (0.007646) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'sgd'}\n",
      "0.844300 (0.007769) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.848800 (0.008280) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'sgd'}\n",
      "0.846400 (0.007946) with: {'batch_size': 128, 'epochs': 200, 'init': 'glorot_normal', 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: how to pass callable OCC.loss to CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Integer encoding\n",
    "\n",
    "Compare cross entropy vs mse; output dim = 1 and output activation is relu (or sigmoid)\n",
    "\n",
    "**TODO**: create pipeline that first compiles and tests model with categorical encoding, then compiles and tests model with integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_int = KerasClassifier(build_fn=create_model, p=P, k=1, output_act='relu', verbose=0)\n",
    "\n",
    "grid_int = GridSearchCV(estimator=model_int, param_grid=param_grid, cv=n_folds)\n",
    "\n",
    "# integer encoding\n",
    "grid_int_result = grid_int.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sparse integer encoding\n",
    "\n",
    "Dimension of output layer is K, activation is relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal crossentropy loss\n",
    "model.compile(loss=OCC.loss,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y_cat, epochs=20, batch_size=128)\n",
    "score = model.evaluate(test_x, test_y_cat, batch_size=128)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Sparse integer encoding is actually the worst!\n",
    "- Accuracy about the same for cross_entropy and ordinal_cross_entropy loss functions + categorical encoding\n",
    "\n",
    "This is still using the default network settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
